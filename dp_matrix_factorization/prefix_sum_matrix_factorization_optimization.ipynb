{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgYDkIhdN0y9"
      },
      "source": [
        "Copyright 2022 The DP Matrix Factorization Authors.\n",
        "Licensed under the Apache License, Version 2.0\n",
        "In this notebook we implement three different methods for computing optimal factorizations for [the matrix mechanism](https://people.cs.umass.edu/~mcgregor/papers/15-vldbj.pdf) under approximate differential privacy:\n",
        "\n",
        "1. Gradient descent on an associated convex problem\n",
        "1. The fixed-point iteration method described in the paper associated to this code.\n",
        "1. A Newton-direction-based algorithm designed in [previous literature](https://arxiv.org/pdf/1602.04302v1.pdf).\n",
        "\n",
        "We experimentally compare their numerical efficiency on the problem of factorizing the prefix-sum matrix: the lower-triangular matrix of all 1s, which takes a vector to its vector of partial sums. Other matrices can be used, and will generally produce similar results.\n",
        "\n",
        "One note on initialization below: it is difficult to initialize the two descent-based schemes and the fixed-point based algorithm identically. In order to ensure similar initialization, it is easiest to select a *vector* (corresponding to the parameterization of the fixed-point algorithm), and attempt to generate a matrix from this vector. Generating this matrix, however, essentially takes advantage of the representations which yield the fixed-point problem--and this generated matrix usually has significantly lower loss than a general positive definite matrix with constant 1s on the diagonal. This observation is not necessarily surprising; the dimensionalities involved in a vector parameterization are much lower. It is slightly unfair to the fixed-point method to 'allow' the gradient-based methods to use this initialization; but since the fixed-point method is the one we propose, we reserve the right to make it look slightly worse than it otherwise might."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1Z1q8omN1sc"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Tuple\n",
        "\n",
        "import time\n",
        "\n",
        "import jax\n",
        "from jax import numpy as jnp\n",
        "from jax import value_and_grad, jit\n",
        "from jax.config import config\n",
        "import numpy as np\n",
        "\n",
        "# With large matrices, the extra precision afforded by performing all\n",
        "# computations in float64 is critical.\n",
        "config.update('jax_enable_x64', True)\n",
        "\n",
        "\n",
        "@jit\n",
        "def diagonalize_and_take_jax_matrix_sqrt(matrix: jnp.ndarray, min_eval: float = 0.0) -\u003e jnp.ndarray:\n",
        "  \"\"\"Matrix square root for positive-semi-definite, Hermitian matrices.\"\"\"\n",
        "  evals, evecs = jnp.linalg.eigh(matrix)\n",
        "  eval_sqrt = jnp.maximum(evals, min_eval)**0.5\n",
        "  sqrt = evecs @ jnp.diag(eval_sqrt) @ evecs.T\n",
        "  return sqrt\n",
        "\n",
        "def hermitian_adjoint(matrix: jnp.ndarray) -\u003e jnp.ndarray:\n",
        "  return jnp.conjugate(matrix).T\n",
        "\n",
        "def compute_loss_in_x(target: jnp.ndarray, x: jnp.ndarray):\n",
        "  m = hermitian_adjoint(target) @ target @ jnp.linalg.inv(x)\n",
        "  raw_trace = jnp.trace(m)\n",
        "  max_diag = jnp.max(jnp.diag(x))\n",
        "  return raw_trace * max_diag\n",
        "\n",
        "def compute_normalized_x_from_vector(matrix_to_factorize, v, precomputed_sqrt: Optional[jnp.ndarray] = None):\n",
        "  \"\"\"Computes a normalized (to all-1s diagonal) version of the vector -\u003e matrix\n",
        "  mapping which defines the relationship between fixed points and optima.\n",
        "\n",
        "  At a fixed point of phi (equivalently an optimum of the symmetrized\n",
        "  factorization problem), the normalization below will no-op. But to normalize\n",
        "  between iterations of the fixed-point method and iterations of the descent-\n",
        "  based methods, it is useful to force the results of this transformation\n",
        "  to always have constant-1 diagonals.\n",
        "  \"\"\"\n",
        "  inv_diag_sqrt = jnp.diag(v ** -(0.5))\n",
        "  diag_sqrt = jnp.diag(v ** 0.5)\n",
        "  if precomputed_sqrt is None:\n",
        "    target = hermitian_adjoint(matrix_to_factorize) @ matrix_to_factorize\n",
        "    matrix_sqrt = diagonalize_and_take_jax_matrix_sqrt(\n",
        "        diag_sqrt @ target.astype(diag_sqrt.dtype) @ diag_sqrt)\n",
        "  else:\n",
        "    # We simply assume that our caller did this computation correctly.\n",
        "    matrix_sqrt = precomputed_sqrt\n",
        "  x = inv_diag_sqrt @ matrix_sqrt @ inv_diag_sqrt\n",
        "  # Force all-1s on diagonal. This normalization is a requirement for the\n",
        "  # initial iterates of the descent-based methods, and we know it's true at the\n",
        "  # optimum.\n",
        "  x_sqrt_diag = jnp.diag(x)\n",
        "  normalized_x = jnp.diag((x_sqrt_diag)**(-0.5)) @ x @ jnp.diag((x_sqrt_diag)**(-0.5))\n",
        "  return normalized_x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q06vyKu8DqKN"
      },
      "source": [
        "# Algorithm implementation: gradient descent on the convex problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GonuoXsN6gP"
      },
      "outputs": [],
      "source": [
        "def optimize_factorization_grad_descent(target: jnp.ndarray, n_iters: int, initial_x: jnp.ndarray, lr: float = 1., use_armijo_rule: bool = True):\n",
        "  \"\"\"Uses JAX-implemented gradient descent to optimize DP-MatFac problem.\"\"\"\n",
        "\n",
        "  # Capture target in loss definition.\n",
        "  compute_loss = lambda x: compute_loss_in_x(target=target, x=x)\n",
        "  compiled_loss = jit(compute_loss)\n",
        "\n",
        "  def find_next_iterate(x_iter, grad, init_lr):\n",
        "    candidate = x_iter - grad * init_lr\n",
        "    non_pd = jnp.any(jnp.isnan(jnp.linalg.cholesky(candidate)))\n",
        "    if non_pd:\n",
        "      # We choose 0.1 as the Armijo factor; this is what the paper we're looking to reproduce does as well\n",
        "      return find_next_iterate(x_iter, grad, init_lr * 0.1)\n",
        "    else:\n",
        "      sufficient_decrease_condition = compiled_loss(x_iter) + init_lr * 0.25 * jnp.sum(grad ** 2)\n",
        "      if compiled_loss(candidate) \u003c= sufficient_decrease_condition:\n",
        "        return candidate\n",
        "      return find_next_iterate(x_iter, grad, init_lr * 0.1)\n",
        "\n",
        "  loss_and_grad = value_and_grad(compute_loss)\n",
        "\n",
        "  x_iter = initial_x\n",
        "\n",
        "  loss_array = []\n",
        "  time_array = []\n",
        "\n",
        "  start = time.time()\n",
        "  for i in range(n_iters):\n",
        "    # Gradient step\n",
        "    loss, grad = loss_and_grad(x_iter)\n",
        "    diag_elements = jnp.diag_indices_from(grad)\n",
        "    grad1 = grad.at[diag_elements].set(0)\n",
        "    loss_array.append(loss)\n",
        "    if use_armijo_rule:\n",
        "      x_iter = find_next_iterate(x_iter, grad1, lr)\n",
        "    else:\n",
        "      x_iter = x_iter - lr * grad1\n",
        "    # Orthogonally project onto symmetric matrices.\n",
        "    x_iter = (x_iter + x_iter.T) / 2\n",
        "    \n",
        "    time_array.append(time.time() - start)\n",
        "  \n",
        "  # Suppress any costs to the first iteration\n",
        "  initial_time = time_array[0]\n",
        "  time_array = [x - initial_time for x in time_array]\n",
        "  return x_iter, loss_array, time_array\n",
        "\n",
        "s_matrix = jnp.tril(jnp.ones(shape=(128, 128)))\n",
        "opt, losses, time_in_loop = optimize_factorization_grad_descent(s_matrix, 100, jnp.eye(s_matrix.shape[0]), lr=1.)\n",
        "\n",
        "print(f'Time in loop: {time_in_loop}')\n",
        "print(opt)\n",
        "print(losses)\n",
        "print(jnp.min(jnp.linalg.eigh(opt)[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omsdtGuY2REV"
      },
      "source": [
        "# Algorithm implementation: fixed-point iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVr9OPcvVY1t"
      },
      "outputs": [],
      "source": [
        "def compute_phi_fixed_point(\n",
        "    matrix: jnp.ndarray,\n",
        "    initial_v: jnp.array,\n",
        "    rtol: float = 1e-5,\n",
        "    max_iterations: Optional[int] = None,\n",
        ") -\u003e Tuple[jnp.ndarray, int, float]:\n",
        "\n",
        "  target = hermitian_adjoint(matrix) @ matrix \n",
        "  v = initial_v\n",
        "\n",
        "  n_iters = 0\n",
        "\n",
        "  def continue_loop(iteration: int) -\u003e bool:\n",
        "    if max_iterations is None:\n",
        "      return True\n",
        "    return iteration \u003c max_iterations\n",
        "\n",
        "  @jit\n",
        "  def _compute_loss(v, matrix_sqrt):\n",
        "    # This computation in the middle may slow down our fixed point method and could\n",
        "    # be bypassed. We only have it here to track the loss as we iterate.\n",
        "    normalized_x = compute_normalized_x_from_vector(matrix, v, matrix_sqrt)\n",
        "    loss = compute_loss_in_x(matrix, normalized_x)\n",
        "    return loss\n",
        "\n",
        "  def _update_loss(v, matrix_sqrt):\n",
        "    loss = _compute_loss(v, matrix_sqrt)\n",
        "    # We rely on Python late binding to capture start here.\n",
        "    time_array.append(time.time() - start)\n",
        "    loss_array.append(loss)\n",
        "\n",
        "  time_array = []\n",
        "  loss_array = []\n",
        "  # We keep around the previously computed matrix square root\n",
        "  # to save time in evaluating loss.\n",
        "  matrix_sqrt = diagonalize_and_take_jax_matrix_sqrt(jnp.diag(v) ** 0.5 @ target @ jnp.diag(v) ** 0.5)\n",
        "\n",
        "  start = time.time()\n",
        "  while continue_loop(n_iters):\n",
        "    n_iters += 1\n",
        "    # Compute loss first, for first iteration, to normalize the loss trajectories\n",
        "    # between these loss arrays and the descent-based ones.\n",
        "    _update_loss(v, matrix_sqrt)\n",
        "    diag = jnp.diag(v)\n",
        "    diag_sqrt = diag ** 0.5\n",
        "    new_v = jnp.diag(matrix_sqrt)\n",
        "    # Set up matrix_sqrt for the next iteration. We use this wonky update order to be\n",
        "    # able to cache this square root computation for loss evaluation.\n",
        "    matrix_sqrt = diagonalize_and_take_jax_matrix_sqrt(jnp.diag(new_v) ** 0.5 @ target @ jnp.diag(new_v) ** 0.5)\n",
        "    norm_diff = jnp.linalg.norm(new_v - v)\n",
        "    rel_norm_diff = norm_diff / jnp.linalg.norm(v)\n",
        "    if rel_norm_diff \u003c rtol:\n",
        "      _update_loss(new_v, matrix_sqrt)\n",
        "      return new_v, n_iters, rel_norm_diff, time_array, loss_array\n",
        "    v = new_v\n",
        "\n",
        "  _update_loss(v, matrix_sqrt)\n",
        "  return v, n_iters, rel_norm_diff, time_array, loss_array\n",
        "\n",
        "def optimize_factorization_fixed_point(s_matrix: jnp.ndarray, max_iterations: int, rtol: float, initial_v: Optional[jnp.array]=None):\n",
        "  if initial_v is None:\n",
        "    initial_v = jnp.ones_like(jnp.diag(s_matrix))\n",
        "  (lagrange_multiplier, n_iters,\n",
        "   final_relnorm, timing, losses) = compute_phi_fixed_point(\n",
        "       s_matrix, rtol=rtol, max_iterations=max_iterations, initial_v=initial_v)\n",
        "   \n",
        "  inv_diag_sqrt = jnp.diag(lagrange_multiplier**-(0.5))\n",
        "  diag_sqrt = jnp.diag(lagrange_multiplier**0.5)\n",
        "\n",
        "  target = hermitian_adjoint(s_matrix) @ s_matrix\n",
        "  x = inv_diag_sqrt @ diagonalize_and_take_jax_matrix_sqrt(\n",
        "      diag_sqrt @ target.astype(diag_sqrt.dtype) @ diag_sqrt) @ inv_diag_sqrt\n",
        "\n",
        "  # Suppress any costs to the first iteration, often due to tracing, etc\n",
        "  initial_time = timing[0]\n",
        "  adj_time_array = [x - initial_time for x in timing]\n",
        "\n",
        "  return x, losses, adj_time_array\n",
        "\n",
        "\n",
        "opt, loss, time_to_compute = optimize_factorization_fixed_point(s_matrix, 2, rtol=1e-1)\n",
        "print(f'Opt array: {opt}')\n",
        "print(f'Time to compute: {time_to_compute}')\n",
        "print(f'losses: {loss}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY1nq1rasK2w"
      },
      "source": [
        "# Newton-step-style algorithm from [existing literature](https://arxiv.org/pdf/1602.04302v1.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IU7CO7TAgEJR"
      },
      "outputs": [],
      "source": [
        "# Implementing Alg 1 from https://arxiv.org/pdf/1602.04302v1.pdf.\n",
        "\n",
        "def compute_newton_direction(Z, grad, max_iter: int = 5):\n",
        "  \"\"\"Implements algorithm 2 from the referenced paper.\"\"\"\n",
        "  # Initialize according to line 4.\n",
        "  D = jnp.zeros_like(grad)\n",
        "  R = -grad + Z @ D @ grad + grad @ D @ Z\n",
        "  # Set diag of D and R to zero; line 5\n",
        "  diag_elements = jnp.diag_indices_from(D)\n",
        "  D = D.at[diag_elements].set(0)\n",
        "  R = R.at[diag_elements].set(0)\n",
        "  # Initialize P and r_old as in line 6.\n",
        "  P = R\n",
        "  # Interestingly, this is the inner product used in the paper.\n",
        "  r_old = jnp.sum(R * R)\n",
        "  for i in range(max_iter):\n",
        "    # Set B and alpha as in line 8\n",
        "    B = -grad + Z @ D @ grad + grad @ D @ Z\n",
        "    alpha = r_old / jnp.sum(P * B)\n",
        "    # Update D and R as in line 9\n",
        "    D = D + alpha * P\n",
        "    R = R - alpha * B\n",
        "    # Set diags of D anr R to 0, as in line 10\n",
        "    D = D.at[diag_elements].set(0)\n",
        "    R = R.at[diag_elements].set(0)\n",
        "    # Set r_new and update P, as in line 11\n",
        "    r_new = jnp.sum(R * R)\n",
        "    P = R + r_new / r_old * P\n",
        "    # Update r_old; line 12\n",
        "    r_old = r_new\n",
        "    if jnp.max(jnp.abs(R)) == 0:\n",
        "      # Everything nans if this is violated. I assume this loop should terminate in this case.\n",
        "      break\n",
        "  return D\n",
        "\n",
        "def optimize_factorization_newton_step(target: jnp.ndarray, n_iters: int, initial_x: jnp.ndarray, init_lr=1.):\n",
        "  \"\"\"Uses JAX-implemented gradient descent to optimize DP-MatFac problem.\"\"\"\n",
        "\n",
        "  # Setting to 1 reproduces the paper of interest; see section 4.2.\n",
        "  # We parameterize for the purposes of tuning.\n",
        "  lr = init_lr\n",
        "\n",
        "  # Capture target in loss definition.\n",
        "  compute_loss = lambda x: compute_loss_in_x(target=target, x=x)\n",
        "  compiled_loss = jit(compute_loss)\n",
        "\n",
        "  def find_next_iterate_armijo(x_iter, grad, newton_dir, init_lr):\n",
        "    \"\"\"Computes step size as in Sec 4.2 of referenced paper.\"\"\"\n",
        "    candidate = x_iter + newton_dir * init_lr\n",
        "    # This is essentially the method for checking positive-definiteness proposed\n",
        "    # by the paper.\n",
        "    non_pd = jnp.any(jnp.isnan(jnp.linalg.cholesky(candidate)))\n",
        "    if non_pd:\n",
        "      # We choose 0.1 as the Armijo factor; this is what the paper we're looking to reproduce does as well\n",
        "      return find_next_iterate_armijo(x_iter, grad, newton_dir, init_lr * 0.1)\n",
        "    # Equation (16)\n",
        "    target_decrease = compiled_loss(x_iter) + init_lr * 0.25 * jnp.sum(grad * newton_dir)\n",
        "    if compiled_loss(candidate) \u003c= target_decrease:\n",
        "      return candidate\n",
        "    return find_next_iterate_armijo(x_iter, grad, newton_dir, init_lr * 0.1)\n",
        "\n",
        "  loss_and_grad = value_and_grad(compiled_loss)\n",
        "\n",
        "  x_iter = initial_x\n",
        "\n",
        "  loss_array = []\n",
        "  time_array = []\n",
        "\n",
        "  start = time.time()\n",
        "  for i in range(n_iters):\n",
        "    # Gradient step\n",
        "    loss, grad = loss_and_grad(x_iter)\n",
        "    inv_x = jnp.linalg.inv(x_iter)\n",
        "    newton_direction = compute_newton_direction(Z=inv_x, grad=grad)\n",
        "    loss_array.append(loss)\n",
        "    x_iter = find_next_iterate_armijo(x_iter, grad, newton_direction, lr)\n",
        "    # Orthogonally project onto symmetric matrices.\n",
        "    x_iter = (x_iter + x_iter.T) / 2 \n",
        "    time_array.append(time.time() - start)\n",
        "\n",
        "  # Suppress any costs to the first iteration\n",
        "  initial_time = time_array[0]\n",
        "  time_array = [x - initial_time for x in time_array]\n",
        "  return x_iter, loss_array, time_array\n",
        "\n",
        "opt, losses, time_in_loop = optimize_factorization_newton_step(s_matrix, 100, jnp.eye(s_matrix.shape[0]))\n",
        "print(f'Final loss: {losses}')\n",
        "print(f'Time in loop: {time_in_loop}')\n",
        "print(f'Min eval: {jnp.min(jnp.linalg.eigh(opt)[0])}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dseTAZa-Dm9t"
      },
      "source": [
        "# Data and plot generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekxkv8DDMKqq"
      },
      "outputs": [],
      "source": [
        "_MAX_ITERS = 1000\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "def matrix_factorization_speed_data(matrix, dim: int, max_inv_rtol: int = 20, max_iter: int = 1000):\n",
        "\n",
        "  # We fix a seed for reproducability, though the results are generally uniform\n",
        "  # in this seed.\n",
        "  key = jax.random.PRNGKey(256)\n",
        "  initial_v = jax.random.uniform(key=key, shape=jnp.diag(matrix).shape)\n",
        "\n",
        "  fp_data = {'x': [], 'y': []}\n",
        "  gd_data = {'x': [], 'y': []}\n",
        "  ns_data = {'x': [], 'y': []}\n",
        "\n",
        "  _, losses_fp, time_to_compute_fp = optimize_factorization_fixed_point(matrix, _MAX_ITERS, rtol=10**-max_inv_rtol, initial_v=initial_v)\n",
        "  fp_data['x'] = time_to_compute_fp\n",
        "  fp_data['y'] = [float(x.to_py()) for x in losses_fp]\n",
        "\n",
        "  initial_x = compute_normalized_x_from_vector(matrix, initial_v)\n",
        "\n",
        "  n_iters = max_iter\n",
        "  # True gradient descent on the convex problem\n",
        "  _, losses, time_in_loop = optimize_factorization_grad_descent(matrix, n_iters, initial_x, lr=1., use_armijo_rule=True)\n",
        "  gd_data['x'] = time_in_loop\n",
        "  gd_data['y'] = [float(x.to_py()) for x in losses]\n",
        "\n",
        "  # The Newton-direction-based method of https://arxiv.org/pdf/1602.04302v1.pdf\n",
        "  _, losses, time_in_loop = optimize_factorization_newton_step(matrix, n_iters, initial_x, init_lr=1.)\n",
        "  ns_data['x'] = time_in_loop\n",
        "  ns_data['y'] = [float(x.to_py()) for x in losses]\n",
        "\n",
        "  df = pd.DataFrame({'Elapsed Time (s)': fp_data['x'] + gd_data['x'] + ns_data['x'], \n",
        "                   'Loss': fp_data['y'] + gd_data['y'] + ns_data['y'],\n",
        "                   'Method': ['Fixed point'] * len(fp_data['x']) + ['Gradient descent'] * len(gd_data['x']) + ['Newton-based step'] * len(ns_data['x'])})\n",
        "  return df\n",
        "\n",
        "\n",
        "def prefix_sum_factorization_speed_data(dim: int, max_inv_rtol: int = 20, max_iter: int = 1000) -\u003e pd.DataFrame:\n",
        "  s_matrix = jnp.tril(jnp.ones(shape=(dim, dim)))\n",
        "  return matrix_factorization_speed_data(s_matrix, dim, max_inv_rtol, max_iter)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzVm0QGVSa6g"
      },
      "outputs": [],
      "source": [
        "def generate_and_plot_data(dim: int, max_inv_rtol: int, max_iter: int):\n",
        "\n",
        "  df = prefix_sum_factorization_speed_data(dim=dim, max_inv_rtol=max_inv_rtol, max_iter=max_iter)\n",
        "  palette = sns.color_palette('muted')\n",
        "  plt.figure(figsize=(10, 7))\n",
        "  sns.set_style('whitegrid')\n",
        "  sns.set_context('paper')\n",
        "  line = sns.lineplot(data=df,\n",
        "              x='Elapsed Time (s)',\n",
        "              y='Loss',\n",
        "              hue='Method',\n",
        "              palette=[palette[0], palette[1], palette[2]],\n",
        "              )\n",
        "\n",
        "  # Compute the max time that all methods generated data for.\n",
        "  max_elapsed_times = []\n",
        "  for method in df['Method'].unique():\n",
        "    max_elapsed_times.append(np.max(df[df['Method'] == method]['Elapsed Time (s)']))\n",
        "  max_all_elapsed_time = max(max_elapsed_times)\n",
        "\n",
        "  max_loss = np.max(df['Loss'])\n",
        "  min_loss = np.min(df['Loss'])\n",
        "\n",
        "  # Heuristic method to set the ranges for visibility\n",
        "  plt.ylim(min_loss - (max_loss - min_loss) * 0.05, max_loss)\n",
        "  plt.xlim(0, max_all_elapsed_time)\n",
        "  return df \n",
        "\n",
        "df = generate_and_plot_data(2048, 10, 1000)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "name": "Prefix-sum matrix factorization optimization.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1eY_VCuTGPrvwhGEH8f3QKNzAtAw-BkPC",
          "timestamp": 1653448381403
        },
        {
          "file_id": "1Wfhb9XY3uGWfA8jm2gYdx28JhODLwcho",
          "timestamp": 1652721590159
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
